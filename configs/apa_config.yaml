# General configuration for hyperspectral image processing and CNN-based analysis
config:
  data:
    input_path: "data/hyperspectral_images/"        # Path to hyperspectral images
    output_path: "results/"                         # Directory for output results
    bands_to_use: [10, 20, 30, 40, 50]              # Selected spectral bands for processing
    spatial_resolution: 1.0                         # Resolution of the image in meters
    label_file: "data/labels.csv"                   # Path to labels for supervised learning
    rois: [[-83.14294, -83.00007,42.34429, 42.39170]] # ROI to crop the full hyperspectral image and its X,Y in format of [xmin_cut, xmax_cut, ymin_cut, ymax_cut]

  preprocessing:
    normalization: true                           # Whether to normalize pixel values
    normalization_type: "min-max"                 # Type of normalization (min-max, z-score)
    spectral_smoothing: false                     # Whether to apply spectral smoothing
    georeferencing:                               # Georeferencing to apply
      merge_thrshld: [0.05]
    augmentations:                                # Augmentations to apply
      rotation: true                              # Enable random rotations
      flipping: true                              # Enable random horizontal/vertical flips
      cropping: true                              # Enable random cropping
      crop_size: [64, 64]                         # Size of the cropped patches (height, width)

  cnn_model:
    architecture: "3D-CNN"                        # Type of CNN architecture (e.g., 3D-CNN, 2D-CNN)
    input_shape: [64, 64, 10]                     # Shape of input tensor (height, width, bands)
    num_classes: 5                                # Number of target classes for classification
    base_filters: 32                              # Number of filters in the first layer
    dropout_rate: 0.5                             # Dropout rate for regularization
    activation_function: "relu"                   # Activation function to use
    optimizer: "adam"                             # Optimizer for training
    loss_function: "categorical_crossentropy"     # Loss function for training

  training:
    batch_size: 16                                # Batch size for training
    epochs: 50                                    # Number of epochs to train the model
    learning_rate: 0.001                          # Initial learning rate
    learning_rate_schedule:                       # Learning rate schedule
      type: "exponential_decay"                   # Type of schedule (e.g., step_decay, exponential_decay)
      decay_rate: 0.95                            # Decay rate for the learning rate
      decay_steps: 10                             # Number of steps before applying decay
    validation_split: 0.2                         # Fraction of the dataset to use for validation

  output:
    save_model: true                              # Whether to save the trained model
    model_save_path: "models/trained_model.h5"    # Path to save the trained model
    log_path: "logs/training.log"                 # Path for training logs
    evaluation_metrics:                           # Metrics to evaluate during and after training
      - "accuracy"
      - "precision"
      - "recall"
      - "f1_score"

